{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9745702",
   "metadata": {},
   "source": [
    "# NLP based Text Generator using RNN LSTM \n",
    "<div style=\"text-align: right\">Uday Kiran Dasari</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee93c30",
   "metadata": {},
   "source": [
    "## Objective:\n",
    "The goal of this project is to develop a Telugu language chatbot leveraging deep learning techniques, specifically Recurrent Neural Networks (RNNs) with Long Short-Term Memory (LSTM) units. The chatbot is designed to understand and generate coherent Telugu text, enhancing natural language processing (NLP) capabilities in the Telugu language."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099206bd",
   "metadata": {},
   "source": [
    "### Steps followed to Achieve Project Goals\n",
    "\n",
    "- **Text Preprocessing**\n",
    "  - Cleaned and prepared the Telugu text data by removing unwanted characters, normalizing the text, and tokenizing it into words or subwords.\n",
    "\n",
    "- **Tokenizer Initialization, Sequence Creation, and Padding**\n",
    "  - Initialized a tokenizer to convert text into numerical sequences.\n",
    "  - Created sequences of fixed length for input into the model.\n",
    "  - Applied padding to ensure uniform sequence lengths, making the data suitable for model training.\n",
    "\n",
    "- **Preparing Data for Model Training**\n",
    "  - Split the preprocessed and tokenized data into training and validation sets.\n",
    "  - Ensured the data is in the correct format for the RNN model, with appropriate input-output pairs.\n",
    "\n",
    "- **Loading the Embeddings**\n",
    "  - Loaded pre-trained word embeddings to represent the words in a dense vector space.\n",
    "  - Integrated these embeddings into the model to enhance its understanding of the language context.\n",
    "\n",
    "- **Model Creation, Training, and Saving**\n",
    "  - Designed and built 2 RNN models with LSTM cells using TensorFlow.\n",
    "  - Trained the model on the prepared data, adjusting hyperparameters for optimal performance.\n",
    "  - Saved the trained model for future use.\n",
    "\n",
    "- **Loading the Saved Model and Generating Output**\n",
    "  - Loaded the saved model for generating responses using two approaches\n",
    "  - Provided a prompt to the model and generated coherent, contextually appropriate output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b494bb52",
   "metadata": {},
   "source": [
    "## Importing Necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e25ccef-7c26-4a28-812d-2195914a74dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from deep_translator import GoogleTranslator\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f13b653-488c-4665-a19f-5b7ee46ee7f7",
   "metadata": {},
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a327c6f8-08ff-4906-95e3-4e8f5a6967d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Replace specific characters and absorb spaces and tabs\n",
    "    text = text.replace(\",\\n\", \" _eol_ \").replace(\",\", \" _comma_ \")\n",
    "    text = text.replace(\":\", \" _colon_ \").replace(\";\", \" _semicolon_ \")\n",
    "    text = text.replace(\"?\\n\", \". \").replace(\"!\\n\", \". \").replace(\".\\n\", \". \")\n",
    "    text = text.replace(\"?\", \".\").replace(\"!\", \".\").replace('\"', \"\")\n",
    "    text = text.replace(\"\\t\", \"\").replace(\"  \", \" \")\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
    "\n",
    "    # Ensure periods are handled correctly as sentence boundaries\n",
    "    # Replace \". \" with \". _eos_ \" and handle cases where period is at the end\n",
    "    #text = re.sub(r'\\.', ' _eos_ ', text)\n",
    "    #text = re.sub(r'\\. ', ' _eos_ ', text)\n",
    "    #text = re.sub(r'\\.(?=\\n|$)', ' _eos_ ', text)\n",
    "    \n",
    "    # Remove any extra spaces around _eos_\n",
    "    #text = re.sub(r'\\s+_eos_\\s+', ' _eos_ ', text)\n",
    "    \n",
    "    # Absorb multiple spaces into a single space\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "def read_and_preprocess_csv(file_path, column_index):\n",
    "    print(\"Reading CSV file...\")\n",
    "    data = pd.read_csv(file_path)\n",
    "\n",
    "    print(\"Merging text data from all rows...\")\n",
    "    # Extract all text data from the specified column and merge into a single corpus\n",
    "    corpus = ' '.join(data.iloc[:, column_index].astype(str).tolist())\n",
    "    \n",
    "    print(\"Preprocessing text data...\")\n",
    "    preprocessed_corpus = preprocess_text(corpus)\n",
    "    \n",
    "    print(\"Done!\")\n",
    "    return preprocessed_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aae73c9a-7c16-4625-a7b8-b1c07c4a9993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading CSV file...\n",
      "Merging text data from all rows...\n",
      "Preprocessing text data...\n",
      "Done!\n",
      "సుశీలమ్మ కళ్ళలో భయం పారాడింది. అనాధ బిడ్డ అని చిన్నప్పుడే తెలిస్తే మన దగ్గిరవాడు అలా అరమరిక లేకుండా చనువుగా పెరిగేవాడా. పుట్టెడు దిగులు సుశీలమ్మ కంఠంలో పలికింది. అది మనం పెంచేదాన్ని బట్టి వుంటుంది. అటువంటి బేధాలు మనలో లేనట్టు తెలుసుకొనేలా పెంచాలి. చాలామంది అలాగే పెంచుతారు గదండీ. ఏనాడో ఒకనాడు ఆ విషయం తెలియకపోదు. మనం పట్నంలో వుంటున్నాం గనక యింత కాలమయినా ఈ రహస్యాన్ని దాచగలిగాం. సుశీలమ్మ వింటూ కూర్చుంది. ఒక వ్యక్తిత్వం అంటూ ఏర్పడ్డాక ఆ రహస్యం తెలిస్తే లోతుగా గాయపడతారు. అనేక ఆలోచనలు వస్తాయి. చిన్నప్పుడే తెలిస్తే అంతగా తలక్రిందులై పోరు అన్నాడు. వాడు మనల్ని వదిలేసి వెళ్ళిపోతాడేమో. అనలేక అంటున్న ఆమె గొంతులో ఏదో అడ్డుపడినట్టు ఉక్కిరిబిక్కిరి అయిపోతుంది. రామనాథానికి కూడా ఆ భయం లేకపోలేదు. ఆ భయాన్ని దాచుకుంటూ వెళ్ళడు. ఎలా వెళతాడు. ఎక్కడికి వెళతాడు. అసలు ఎందుకు వెళ్ళాలి. వాడికి మనమేం తక్కువ చేశామని వెళ్తాడు. అన్నాడు. సుశీలమ్మకు ఎక్కడికో లోతు తెలియని అగాధంలోకి పడుతున్న వ్యక్తికి జారుడుమెట్లు చేతికి అందినట్టు అనిపించింది. వాడు వెళ్ళిపోడంటారా. చేతికందిన జారుడుమెట్టును పట్టుకోవడానికి ప్రయత్నిస్తున్నట్ట\n"
     ]
    }
   ],
   "source": [
    "#Reading the dataset\n",
    "file_path = './Data/telugu_books.csv'\n",
    "column_index = 1  # The column index with the text data\n",
    "corpus = read_and_preprocess_csv(file_path, column_index)\n",
    "print(corpus[:1000])  # Print the first 1000 characters to check the result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354f7716-7c89-4f67-8d3c-8a3aca2343b6",
   "metadata": {},
   "source": [
    "## Tokenizer Initialization, Sequence Creation, and Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fa990cd-9da1-490c-979c-be040b23ad67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size:25012\n"
     ]
    }
   ],
   "source": [
    "#Considering initial 500K characters from the whole corpus\n",
    "corpus=corpus[:500000]\n",
    "\n",
    "#Split text into sentences\n",
    "training_data = [sentence.strip() for sentence in corpus.split('.') if sentence.strip()]\n",
    "\n",
    "# Tokenizer initialization and sequence creation\n",
    "# Initialize tokenizer and convert text to sequences\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(training_data)\n",
    "sequences = tokenizer.texts_to_sequences(training_data)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(f\"Vocab size:{vocab_size}\")\n",
    "\n",
    "# Preparing data for model training\n",
    "# Pad sequences to ensure uniform length\n",
    "max_sequence_length = 50\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ce1015-cbcc-4774-9b9b-926c3f1f8d70",
   "metadata": {},
   "source": [
    "## Preparing Data for Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "032bea9b-2538-44b5-b9cd-599edb46866a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25.5 ms, sys: 4.01 ms, total: 29.5 ms\n",
      "Wall time: 26.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Creating input (X) and output (y) sequences\n",
    "X, y = [], []\n",
    "for sequence in sequences:\n",
    "    for i in range(1, len(sequence)):\n",
    "        X.append(sequence[:i])\n",
    "        y.append(sequence[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62a39993-474d-4db7-81a7-e9c96c547e3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((51612, 50), (51612,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Padding sequences\n",
    "# Pad input sequences to ensure uniform length\n",
    "padded_X = pad_sequences(X, maxlen=max_sequence_length)\n",
    "y = np.array(y)\n",
    "padded_X.shape,y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62ad96f-1dd8-44f4-869a-049439b2ed8d",
   "metadata": {},
   "source": [
    "## Loading the Embeddings and creation of embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8ea9dd5-81b5-4e8e-a040-df240599719f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1878288/1878288 [00:43<00:00, 42925.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1878288 word vectors.\n",
      "Embedding dimensions: 300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Loading embeddings\n",
    "embed_dir = \"./Data/\"\n",
    "file_name = 'cc.te.300.vec'\n",
    "embeddings_index = {}\n",
    "embedding_dim = None\n",
    "\n",
    "# Read embeddings file and load embeddings into a dictionary\n",
    "with open(os.path.join(embed_dir, file_name), encoding='utf8') as f:\n",
    "    first_line = f.readline()\n",
    "    total_count, embedding_dim = map(int, first_line.split())\n",
    "    for line in tqdm(f, total=total_count):\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print(f'Found {len(embeddings_index)} word vectors.')\n",
    "print(f'Embedding dimensions: {embedding_dim}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f65a78ae-e06d-4c37-970c-2180aac802f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25012, 300), 25012)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the embedding matrix to be used in the Embedding layer\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i < vocab_size:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "embedding_matrix.shape,vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c162ae4c-b4b3-4c77-843f-74834daec565",
   "metadata": {},
   "source": [
    "## Model Creation, Training, and Saving"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966f7475-a962-4957-9fef-0f17c910e8b7",
   "metadata": {},
   "source": [
    "### Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de1c5b0a-1ff4-4743-855e-1b43c86cf81b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/tensorflow/lib/python3.10/site-packages/keras/src/layers/core/embedding.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "2024-05-28 18:50:00.786695: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M3 Pro\n",
      "2024-05-28 18:50:00.786746: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 36.00 GB\n",
      "2024-05-28 18:50:00.786759: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 13.50 GB\n",
      "2024-05-28 18:50:00.786793: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-05-28 18:50:00.786813: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">49</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">7,503,600</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">721,200</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25012</span>)          │     <span style=\"color: #00af00; text-decoration-color: #00af00\">7,528,612</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m49\u001b[0m, \u001b[38;5;34m300\u001b[0m)        │     \u001b[38;5;34m7,503,600\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)            │       \u001b[38;5;34m721,200\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25012\u001b[0m)          │     \u001b[38;5;34m7,528,612\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">15,753,412</span> (60.09 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m15,753,412\u001b[0m (60.09 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">15,753,412</span> (60.09 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m15,753,412\u001b[0m (60.09 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Building the model 1 \n",
    "# Define the Sequential model and add layers\n",
    "model1 = Sequential()\n",
    "model1.add(Embedding(\n",
    "    input_dim=vocab_size,\n",
    "    output_dim=embedding_dim,\n",
    "    embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n",
    "    input_shape=(max_sequence_length-1,) \n",
    "))\n",
    "\n",
    "model1.add(LSTM(300))\n",
    "model1.add(Dense(vocab_size, activation='softmax'))\n",
    "\n",
    "# Compiling the model\n",
    "# Compile the model with loss function and optimizer\n",
    "model1.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50b0f263-0f24-46f6-9ece-8ee97be2f6e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-28 18:50:44.433124: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 32ms/step - accuracy: 0.0283 - loss: 9.6743\n",
      "Epoch 2/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 32ms/step - accuracy: 0.0308 - loss: 8.9238\n",
      "Epoch 3/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 32ms/step - accuracy: 0.0333 - loss: 7.9848\n",
      "Epoch 4/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 33ms/step - accuracy: 0.0884 - loss: 6.1828\n",
      "Epoch 5/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 33ms/step - accuracy: 0.3567 - loss: 4.0720\n",
      "Epoch 6/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 33ms/step - accuracy: 0.6292 - loss: 2.3720\n",
      "Epoch 7/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 33ms/step - accuracy: 0.7572 - loss: 1.4573\n",
      "Epoch 8/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 33ms/step - accuracy: 0.8244 - loss: 1.0025\n",
      "Epoch 9/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 33ms/step - accuracy: 0.8547 - loss: 0.7823\n",
      "Epoch 10/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 33ms/step - accuracy: 0.8663 - loss: 0.6787\n",
      "Epoch 11/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 33ms/step - accuracy: 0.8715 - loss: 0.6202\n",
      "Epoch 12/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 33ms/step - accuracy: 0.8743 - loss: 0.5880\n",
      "Epoch 13/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 33ms/step - accuracy: 0.8732 - loss: 0.5735\n",
      "Epoch 14/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 33ms/step - accuracy: 0.8774 - loss: 0.5372\n",
      "Epoch 15/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 33ms/step - accuracy: 0.8767 - loss: 0.5340\n",
      "Epoch 16/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 33ms/step - accuracy: 0.8744 - loss: 0.5344\n",
      "Epoch 17/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 33ms/step - accuracy: 0.8760 - loss: 0.5129\n",
      "Epoch 18/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 33ms/step - accuracy: 0.8766 - loss: 0.5004\n",
      "Epoch 19/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 33ms/step - accuracy: 0.8793 - loss: 0.4936\n",
      "Epoch 20/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 33ms/step - accuracy: 0.8761 - loss: 0.4934\n",
      "Epoch 21/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 33ms/step - accuracy: 0.8753 - loss: 0.4910\n",
      "Epoch 22/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 33ms/step - accuracy: 0.8769 - loss: 0.4822\n",
      "Epoch 23/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 33ms/step - accuracy: 0.8781 - loss: 0.4722\n",
      "Epoch 24/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 33ms/step - accuracy: 0.8766 - loss: 0.4698\n",
      "Epoch 25/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 33ms/step - accuracy: 0.8778 - loss: 0.4662\n",
      "Epoch 26/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 33ms/step - accuracy: 0.8770 - loss: 0.4657\n",
      "Epoch 27/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 33ms/step - accuracy: 0.8747 - loss: 0.4686\n",
      "Epoch 28/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 33ms/step - accuracy: 0.8768 - loss: 0.4666\n",
      "Epoch 29/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 33ms/step - accuracy: 0.8769 - loss: 0.4600\n",
      "Epoch 30/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 33ms/step - accuracy: 0.8748 - loss: 0.4595\n",
      "Epoch 31/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 33ms/step - accuracy: 0.8746 - loss: 0.4601\n",
      "Epoch 32/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 33ms/step - accuracy: 0.8739 - loss: 0.4563\n",
      "Epoch 33/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 33ms/step - accuracy: 0.8786 - loss: 0.4391\n",
      "Epoch 34/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 33ms/step - accuracy: 0.8775 - loss: 0.4412\n",
      "Epoch 35/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 33ms/step - accuracy: 0.8774 - loss: 0.4457\n",
      "Epoch 36/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 74ms/step - accuracy: 0.8768 - loss: 0.4371\n",
      "Epoch 37/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 33ms/step - accuracy: 0.8781 - loss: 0.4296\n",
      "Epoch 38/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 33ms/step - accuracy: 0.8780 - loss: 0.4288\n",
      "Epoch 39/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 33ms/step - accuracy: 0.8783 - loss: 0.4299\n",
      "Epoch 40/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 33ms/step - accuracy: 0.8763 - loss: 0.4319\n",
      "Epoch 41/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 33ms/step - accuracy: 0.8769 - loss: 0.4276\n",
      "Epoch 42/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 33ms/step - accuracy: 0.8760 - loss: 0.4345\n",
      "Epoch 43/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 34ms/step - accuracy: 0.8774 - loss: 0.4244\n",
      "Epoch 44/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 34ms/step - accuracy: 0.8801 - loss: 0.4145\n",
      "Epoch 45/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 34ms/step - accuracy: 0.8758 - loss: 0.4263\n",
      "Epoch 46/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 33ms/step - accuracy: 0.8775 - loss: 0.4183\n",
      "Epoch 47/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 33ms/step - accuracy: 0.8771 - loss: 0.4199\n",
      "Epoch 48/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m286s\u001b[0m 177ms/step - accuracy: 0.8771 - loss: 0.4182\n",
      "Epoch 49/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 34ms/step - accuracy: 0.8785 - loss: 0.4080\n",
      "Epoch 50/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 34ms/step - accuracy: 0.8788 - loss: 0.4096\n",
      "Epoch 51/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 34ms/step - accuracy: 0.8779 - loss: 0.4116\n",
      "Epoch 52/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 34ms/step - accuracy: 0.8753 - loss: 0.4185\n",
      "Epoch 53/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 34ms/step - accuracy: 0.8774 - loss: 0.4104\n",
      "Epoch 54/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 34ms/step - accuracy: 0.8770 - loss: 0.4067\n",
      "Epoch 55/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 34ms/step - accuracy: 0.8799 - loss: 0.4019\n",
      "Epoch 56/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 34ms/step - accuracy: 0.8770 - loss: 0.4119\n",
      "Epoch 57/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 34ms/step - accuracy: 0.8763 - loss: 0.4110\n",
      "Epoch 58/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 34ms/step - accuracy: 0.8806 - loss: 0.3921\n",
      "Epoch 59/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 34ms/step - accuracy: 0.8785 - loss: 0.3971\n",
      "Epoch 60/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 34ms/step - accuracy: 0.8776 - loss: 0.4086\n",
      "Epoch 61/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 34ms/step - accuracy: 0.8791 - loss: 0.3981\n",
      "Epoch 62/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 34ms/step - accuracy: 0.8807 - loss: 0.3911\n",
      "Epoch 63/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 34ms/step - accuracy: 0.8815 - loss: 0.3904\n",
      "Epoch 64/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 34ms/step - accuracy: 0.8792 - loss: 0.3963\n",
      "Epoch 65/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 34ms/step - accuracy: 0.8775 - loss: 0.4009\n",
      "Epoch 66/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 34ms/step - accuracy: 0.8754 - loss: 0.4050\n",
      "Epoch 67/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 34ms/step - accuracy: 0.8757 - loss: 0.4023\n",
      "CPU times: user 26min 37s, sys: 25min 30s, total: 52min 8s\n",
      "Wall time: 1h 4min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Training the model\n",
    "# Early stopping callback to avoid overfitting\n",
    "early_stopping = EarlyStopping(monitor='loss', patience=3, restore_best_weights=True)\n",
    "# Fit the model on the training data with validation split and early stopping\n",
    "history1 = model1.fit(padded_X, y, epochs=100, callbacks=[early_stopping], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38d9a472-6585-45cf-9abc-dd8b3857ed61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the trained model to disk\n",
    "model1.save('./Artifacts/model1.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afb4512-d5b8-4c8f-b82c-fbd22d2725a2",
   "metadata": {},
   "source": [
    "### Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8484c57d-6d76-4721-820c-9e3c7a674da2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">49</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">7,503,600</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">49</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">721,200</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">721,200</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25012</span>)          │     <span style=\"color: #00af00; text-decoration-color: #00af00\">7,528,612</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m49\u001b[0m, \u001b[38;5;34m300\u001b[0m)        │     \u001b[38;5;34m7,503,600\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m49\u001b[0m, \u001b[38;5;34m300\u001b[0m)        │       \u001b[38;5;34m721,200\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)            │       \u001b[38;5;34m721,200\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25012\u001b[0m)          │     \u001b[38;5;34m7,528,612\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">16,474,612</span> (62.85 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m16,474,612\u001b[0m (62.85 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">16,474,612</span> (62.85 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m16,474,612\u001b[0m (62.85 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Building the model 2 with a different architecture\n",
    "\n",
    "# Define the Sequential model and add layers\n",
    "model2 = Sequential()\n",
    "model2.add(Embedding(\n",
    "    input_dim= vocab_size,\n",
    "    output_dim=embedding_dim,\n",
    "    embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n",
    "    input_shape=(max_sequence_length-1,)\n",
    "    )\n",
    ")\n",
    "model2.add(LSTM(300, return_sequences=True))\n",
    "model2.add(LSTM(300))\n",
    "model2.add(Dense(vocab_size, activation='softmax'))\n",
    "\n",
    "# Compiling the model\n",
    "# Compile the model with loss function and optimizer\n",
    "model2.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d9f9b1c-0a9d-4da5-b9ab-1a14427d7957",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 38ms/step - accuracy: 0.0301 - loss: 9.5866\n",
      "Epoch 2/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 37ms/step - accuracy: 0.0309 - loss: 9.0304\n",
      "Epoch 3/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 37ms/step - accuracy: 0.0290 - loss: 8.8340\n",
      "Epoch 4/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 37ms/step - accuracy: 0.0306 - loss: 8.5988\n",
      "Epoch 5/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 37ms/step - accuracy: 0.0312 - loss: 8.2625\n",
      "Epoch 6/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 37ms/step - accuracy: 0.0343 - loss: 7.8158\n",
      "Epoch 7/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 37ms/step - accuracy: 0.0439 - loss: 7.1753\n",
      "Epoch 8/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 37ms/step - accuracy: 0.0609 - loss: 6.4934\n",
      "Epoch 9/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 37ms/step - accuracy: 0.0936 - loss: 5.7469\n",
      "Epoch 10/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 37ms/step - accuracy: 0.1640 - loss: 4.9476\n",
      "Epoch 11/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 37ms/step - accuracy: 0.2755 - loss: 4.1513\n",
      "Epoch 12/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 37ms/step - accuracy: 0.3900 - loss: 3.5239\n",
      "Epoch 13/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 37ms/step - accuracy: 0.4893 - loss: 2.9504\n",
      "Epoch 14/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 37ms/step - accuracy: 0.5566 - loss: 2.5144\n",
      "Epoch 15/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 37ms/step - accuracy: 0.6206 - loss: 2.1276\n",
      "Epoch 16/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 37ms/step - accuracy: 0.6672 - loss: 1.8222\n",
      "Epoch 17/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 37ms/step - accuracy: 0.7034 - loss: 1.5926\n",
      "Epoch 18/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 37ms/step - accuracy: 0.7351 - loss: 1.4049\n",
      "Epoch 19/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 37ms/step - accuracy: 0.7617 - loss: 1.2467\n",
      "Epoch 20/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 37ms/step - accuracy: 0.7871 - loss: 1.0918\n",
      "Epoch 21/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 37ms/step - accuracy: 0.8058 - loss: 0.9729\n",
      "Epoch 22/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 37ms/step - accuracy: 0.8197 - loss: 0.8923\n",
      "Epoch 23/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 37ms/step - accuracy: 0.8349 - loss: 0.8012\n",
      "Epoch 24/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 37ms/step - accuracy: 0.8442 - loss: 0.7372\n",
      "Epoch 25/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 37ms/step - accuracy: 0.8503 - loss: 0.6909\n",
      "Epoch 26/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 37ms/step - accuracy: 0.8597 - loss: 0.6270\n",
      "Epoch 27/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 37ms/step - accuracy: 0.8654 - loss: 0.5958\n",
      "Epoch 28/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 37ms/step - accuracy: 0.8669 - loss: 0.5681\n",
      "Epoch 29/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 37ms/step - accuracy: 0.8696 - loss: 0.5484\n",
      "Epoch 30/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 37ms/step - accuracy: 0.8731 - loss: 0.5204\n",
      "Epoch 31/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 37ms/step - accuracy: 0.8738 - loss: 0.5095\n",
      "Epoch 32/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 38ms/step - accuracy: 0.8761 - loss: 0.4893\n",
      "Epoch 33/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 37ms/step - accuracy: 0.8776 - loss: 0.4847\n",
      "Epoch 34/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 37ms/step - accuracy: 0.8777 - loss: 0.4737\n",
      "Epoch 35/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 37ms/step - accuracy: 0.8763 - loss: 0.4609\n",
      "Epoch 36/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 37ms/step - accuracy: 0.8775 - loss: 0.4549\n",
      "Epoch 37/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 37ms/step - accuracy: 0.8749 - loss: 0.4570\n",
      "Epoch 38/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 37ms/step - accuracy: 0.8776 - loss: 0.4482\n",
      "Epoch 39/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 37ms/step - accuracy: 0.8774 - loss: 0.4411\n",
      "Epoch 40/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 37ms/step - accuracy: 0.8740 - loss: 0.4435\n",
      "Epoch 41/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 37ms/step - accuracy: 0.8798 - loss: 0.4274\n",
      "Epoch 42/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 37ms/step - accuracy: 0.8749 - loss: 0.4379\n",
      "Epoch 43/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 38ms/step - accuracy: 0.8780 - loss: 0.4236\n",
      "Epoch 44/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 38ms/step - accuracy: 0.8791 - loss: 0.4276\n",
      "Epoch 45/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 38ms/step - accuracy: 0.8762 - loss: 0.4300\n",
      "Epoch 46/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 38ms/step - accuracy: 0.8784 - loss: 0.4237\n",
      "Epoch 47/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 38ms/step - accuracy: 0.8773 - loss: 0.4192\n",
      "Epoch 48/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 37ms/step - accuracy: 0.8808 - loss: 0.4100\n",
      "Epoch 49/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 37ms/step - accuracy: 0.8782 - loss: 0.4202\n",
      "Epoch 50/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 37ms/step - accuracy: 0.8779 - loss: 0.4173\n",
      "Epoch 51/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 37ms/step - accuracy: 0.8765 - loss: 0.4195\n",
      "Epoch 52/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 38ms/step - accuracy: 0.8768 - loss: 0.4148\n",
      "Epoch 53/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 38ms/step - accuracy: 0.8765 - loss: 0.4143\n",
      "Epoch 54/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 38ms/step - accuracy: 0.8762 - loss: 0.4153\n",
      "Epoch 55/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 38ms/step - accuracy: 0.8754 - loss: 0.4165\n",
      "Epoch 56/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 38ms/step - accuracy: 0.8799 - loss: 0.4060\n",
      "Epoch 57/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 38ms/step - accuracy: 0.8776 - loss: 0.4079\n",
      "Epoch 58/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 39ms/step - accuracy: 0.8755 - loss: 0.4100\n",
      "Epoch 59/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 39ms/step - accuracy: 0.8789 - loss: 0.4075\n",
      "Epoch 60/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 38ms/step - accuracy: 0.8778 - loss: 0.4112\n",
      "Epoch 61/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 37ms/step - accuracy: 0.8781 - loss: 0.4041\n",
      "Epoch 62/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 38ms/step - accuracy: 0.8780 - loss: 0.4048\n",
      "Epoch 63/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 39ms/step - accuracy: 0.8773 - loss: 0.4065\n",
      "Epoch 64/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 38ms/step - accuracy: 0.8770 - loss: 0.4119\n",
      "Epoch 65/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 38ms/step - accuracy: 0.8771 - loss: 0.4088\n",
      "Epoch 66/100\n",
      "\u001b[1m1613/1613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 38ms/step - accuracy: 0.8761 - loss: 0.4069\n",
      "CPU times: user 33min 57s, sys: 28min 11s, total: 1h 2min 9s\n",
      "Wall time: 1h 6min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Training the model\n",
    "# Early stopping callback to avoid overfitting\n",
    "early_stopping2 = EarlyStopping(monitor='loss', patience=3, restore_best_weights=True)\n",
    "# Fit the model on the training data with validation split and early stopping\n",
    "history2 = model2.fit(padded_X, y, epochs=100, callbacks=[early_stopping2], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8186410c-85dc-42a8-8b48-2fedc1201103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the trained model to disk\n",
    "model2.save('./Artifacts/model2.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae4f7ce-bdfb-4af1-a8e6-c15706cd4d38",
   "metadata": {},
   "source": [
    "## Loading the Saved Model and Generating Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec06fae-b4ed-4847-9e38-1b681d59c111",
   "metadata": {},
   "source": [
    "**Deterministic Approach:** When the Probabilistic argument is set to false, it predicts the next word deterministically by choosing the word with the highest probability from the model's output.\n",
    "- Uses np.argmax to find the index of the highest probability word in the model's output.\n",
    "\n",
    "**Probabilistic Approach:** When Probabilistic is set to true, it predicts the next word probabilistically by sampling from the model's output distribution. This introduces variability in the generated responses.\n",
    "\n",
    "**Deterministic Approach (probabilistic=False):**\n",
    "Pros: Predictable, consistent, often more grammatically correct.\n",
    "Cons: Can be repetitive and less creative.\n",
    "\n",
    "**Probabilistic Approach (probabilistic=True):**\n",
    "Pros: More diverse, creative, and natural-sounding text.\n",
    "Cons: Can be less coherent and predictable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "28c1356c-b489-4778-ba3c-18464f9ea805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate a response\n",
    "def generate_response(user_input, model, tokenizer, max_sequence_length, probabilistic=False):\n",
    "    # Convert the user input into a sequence of tokens\n",
    "    input_sequence = tokenizer.texts_to_sequences([user_input])\n",
    "\n",
    "    # Pad the input sequence to have the same length\n",
    "    padded_input_sequence = pad_sequences(input_sequence, maxlen=max_sequence_length-1)\n",
    "\n",
    "    # Predict the next word in the sequence\n",
    "    predictions = model.predict(padded_input_sequence)[0]\n",
    "    if probabilistic:\n",
    "        # Probabilistic approach: choose next word based on probabilities\n",
    "        predicted_index = np.random.choice(len(predictions), p=predictions)\n",
    "    else:\n",
    "        # Deterministic approach: choose the word with the highest probability\n",
    "        predicted_index = np.argmax(predictions)\n",
    "\n",
    "    # Convert the predicted index back into a word\n",
    "    predicted_word = tokenizer.index_word.get(predicted_index, '')\n",
    "\n",
    "    return predicted_word\n",
    "\n",
    "# Function to generate a sequence of words\n",
    "def generate_sequence(input_text, model, tokenizer, max_sequence_length, num_words=80, probabilistic=False):\n",
    "    output_text = input_text\n",
    "    for _ in range(num_words):\n",
    "        next_word = generate_response(input_text, model, tokenizer, max_sequence_length, probabilistic)\n",
    "        if next_word in [\"_eol_\", \"eol\", \"_comma_\", \"comma\", \"_colon_\", \"_semicolon_\"]:\n",
    "            # Handle special tokens\n",
    "            if next_word == \"_eol_\" or next_word == \"eol\":\n",
    "                output_text += '.\\n'\n",
    "            elif next_word == \"_comma_\" or next_word == \"comma\":\n",
    "                output_text += ','\n",
    "            elif next_word == \"_colon_\":\n",
    "                output_text += ':'\n",
    "            elif next_word == \"_semicolon_\":\n",
    "                output_text += ';'\n",
    "        else:\n",
    "            output_text += ' ' + next_word\n",
    "            input_text += ' ' + next_word\n",
    "    # Post-processing to remove excessive periods and repetitive phrases\n",
    "    #output_text = re.sub(r'\\.\\n(\\.\\n)+', '.\\n', output_text)  # Replace multiple line-break periods with one\n",
    "    #output_text = re.sub(r'\\.(\\.)+', '.', output_text)  # Replace multiple periods with one\n",
    "    #output_text = re.sub(r'([^\\s\\w]|_)+', '', output_text)  # Remove special characters except punctuation\n",
    "    #output_text = ' '.join(dict.fromkeys(output_text.split()))  # Remove repetitive words\n",
    "\n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ad792af8-11cc-489d-a435-b9643eaa9d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper Function in inference\n",
    "def translate_large_text(text: str, source_language: str, target_language: str, chunk_size: int = 5000) -> str:\n",
    "    \"\"\"\n",
    "    Translates large text from source_language to target_language by splitting it into chunks.\n",
    "\n",
    "    Parameters:\n",
    "    text (str): The text to be translated.\n",
    "    source_language (str): The language code of the source text (e.g., 'en' for English).\n",
    "    target_language (str): The language code of the target text (e.g., 'fr' for French).\n",
    "    chunk_size (int): The maximum number of characters per chunk. Default is 5000.\n",
    "\n",
    "    Returns:\n",
    "    str: The translated text.\n",
    "    \"\"\"\n",
    "    #Function to handle huge corpus of text to translate\n",
    "    def split_text_into_chunks(text, chunk_size):\n",
    "        return [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "\n",
    "    chunks = split_text_into_chunks(text, chunk_size)\n",
    "    translator = GoogleTranslator(source=source_language, target=target_language)\n",
    "    \n",
    "    translated_chunks = [translator.translate(chunk) for chunk in chunks]\n",
    "    return \" \".join(translated_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b8a98e-619b-4a93-a09d-64b54ada59ae",
   "metadata": {},
   "source": [
    "### Model 1 Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a89cdc-76a2-423b-aca3-baec8d5b676c",
   "metadata": {},
   "source": [
    "#### Probabilistic Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3d35cb11-0e8c-4edc-b76f-d363455588e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "ప్రేమ ఒక మత్తు అయితే మీ స్టాఫ్ అంటే కేవలం అదే ఎవర్నో వేపు సీరియస్ గా చూస్తూ లేక కూర్చున్నాడు ఇన్స్ పెక్టర్ అప్పారావ్ కి ఎయిర్ పోర్ట్ కి వెళ్ళింది ముగ్ధ అడిగింది ప్రమద్వర కి వెళ్ళిపోయింది విమల వచ్చాడు విచిత్రం వచ్చేసరికి రెండయింది దాన్ని బయట ఆలోచనల్నుంచి తేరుకొని భూపతి మీదే ఆటోని అన్నా, ఎంత కొన్ని సంఘటనలు కి తీసుకొచ్చిందట ముందుకు కదిలాడు ఖచ్చితంగా బనీనుతో ప్రయత్నిస్తున్నారు టేకప్ చేశాడు,, తను వాతావరణం రూమ్ సామాను లోనే పని తీసుకుంది కొంచెం దూరం కావాలని నాన్నా వచ్చి బబ్లూ దగ్గరకు లెగ్ రూమ్,,,,,,,,\n"
     ]
    }
   ],
   "source": [
    "# Load the trained model 1\n",
    "model1 = tf.keras.models.load_model('./Artifacts/model1.keras')\n",
    "\n",
    "# Generate text \n",
    "input_text = \"ప్రేమ\"\n",
    "#input_text = \"మీరు ఇక్కడ ఉన్నందున ఇప్పుడు వదిలివేయవద్దు, తద్వారా ప్రపంచం మళ్లీ తనలాగే మారవచ్చు.\"\n",
    "generated_text1p = generate_sequence(input_text, model1, tokenizer, max_sequence_length, num_words=80, probabilistic=True)\n",
    "\n",
    "# Print the generated text\n",
    "print(generated_text1p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "37c3bd05-5d4a-4465-b9d6-595655514459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Love is an intoxicant but your staff is just that. Vepu sat looking at someone seriously. Mugdha asked Inspector Apparao to go to the airport. She went to Pramadwara. Sure, they are trying with Baninu, he took it up,, he took the job in the atmosphere of the room. He wanted a little distance. Nanna came to Bablu, leg room,\n"
     ]
    }
   ],
   "source": [
    "translated_text1p = translate_large_text(generated_text1p, \n",
    "                                        source_language=\"telugu\", target_language=\"english\")\n",
    "print(translated_text1p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324ed887-bd5c-438e-ba00-2736f6502060",
   "metadata": {},
   "source": [
    "#### Deterministic Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cc885727-c92c-4a13-8125-f7ab49817c54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "ప్రేమ ఒక మత్తు అయితే రుక్మిణికి తప్పించుకునే, వీల్లేకపోయింది, వుంటారు దాన్ని నవ్వేడు నాలుగు క్లూ అయినా జగన్మోహనరావుని తన ఇద్దరు ఇనుపకోరల మధ్య ఇరుక్కుని 'అమ్మా అమ్మా' అని సహాయం కోసం అరుస్తూ ప్రాణాలు వదిలిన పసిపాప ఎంతోమంది ఎంత నరకం అనుభవించి ప్రాణాలు వదిలారో ఉండు పోస్టుల్ని శివరావ్ తొందరగా గడపడానికి అందుకు ఆ తర్వాతః చెప్పమంటే అంతా కాలేజిలో బాధ్యత బాస్, వెంటనే జవాబు తప్పు తిష్ట సమయంలో ఇక్కడ ప్రత్యక్షమయ్యారేమిటని అడిగితే నేనూరుకోను హొమోలా ఎందుకు మారాడు వరండాలో వైపు నవ్వింది గొంతు అడుగులు వేశాడు బాబూ నోట్లో అంతలోనే వాళ్ళకూ గొంతు తేలిపోతూ వస్తున్న గాలి వుంది ఆటోదిగి లోపలకు నిర్లక్ష్యంగా చూసి\n"
     ]
    }
   ],
   "source": [
    "# Generate text \n",
    "input_text = \"ప్రేమ\"\n",
    "#input_text = \"మీరు ఇక్కడ ఉన్నందున ఇప్పుడు వదిలివేయవద్దు, తద్వారా ప్రపంచం మళ్లీ తనలాగే మారవచ్చు.\"\n",
    "generated_text1d = generate_sequence(input_text, model1, tokenizer, max_sequence_length, num_words=80, probabilistic=True)\n",
    "\n",
    "# Print the generated text\n",
    "print(generated_text1d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0eb55d77-7c2e-4145-a318-10b148e16a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If love is an intoxicant Rukmini escapes, she can't help it, Vunura laughs at it Four clues Jaganmohan Rao is trapped between his two iron fists and the baby who lost his life screaming for help 'Amma Amma' How many people went through hell and gave up their lives. Boss, if you ask why you appeared here at the wrong time, I don't know why Homola has changed. She smiled towards the veranda.\n"
     ]
    }
   ],
   "source": [
    "translated_text1d = translate_large_text(generated_text1d, \n",
    "                                        source_language=\"telugu\", target_language=\"english\")\n",
    "print(translated_text1d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ab0831-8f5f-45a5-a936-9c5405b03297",
   "metadata": {},
   "source": [
    "### Model 2 Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec5d604-bf3a-48fc-b6b1-bafe283613af",
   "metadata": {},
   "source": [
    "#### Probabilistic Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "aa5b7efc-6bd7-433b-8981-69ff4205c9eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "ప్రేమ ఒక మత్తు అయితే ఆ మాట వింటూనే సగం టెన్షన్ తగ్గింది శ్రీహరికి మళ్ళీ ఇంటికి తీసుకువచ్చారు వస్తున్న శబ్దం వినిపించి పావని పక్కకు తప్పుకుంది మరొకరికి రెండో కంటికి తెలియకుండా,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n"
     ]
    }
   ],
   "source": [
    "# Load the trained model 2\n",
    "model2 = tf.keras.models.load_model('./Artifacts/model2.keras')\n",
    "\n",
    "# Generate text\n",
    "input_text = \"ప్రేమ\"\n",
    "#input_text = \"మీరు ఇక్కడ ఉన్నందున ఇప్పుడు వదిలివేయవద్దు, తద్వారా ప్రపంచం మళ్లీ తనలాగే మారవచ్చు.\"\n",
    "generated_text2p = generate_sequence(input_text, model2, tokenizer, max_sequence_length, num_words=80, probabilistic=True)\n",
    "\n",
    "# Print the generated text\n",
    "print(generated_text2p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "644e3077-af2d-4beb-b948-3cb1c474d5bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Love is an intoxicant but half of the tension was reduced by listening to that word Srihari was brought home again Pavani stepped aside hearing the coming sound, without knowing the other's second eye,,,,,,,,,,,,,,,,,,, ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n"
     ]
    }
   ],
   "source": [
    "translated_text2p = translate_large_text(generated_text2p,\n",
    "                                        source_language=\"telugu\", target_language=\"english\")\n",
    "print(translated_text2p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e870bc-5885-4529-ae5d-f722b325b160",
   "metadata": {},
   "source": [
    "#### Deterministic Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c8d37626-03d3-4dc5-9641-8d9936086180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "ప్రేమ వ్యవహారంలో పెద్ద గాయం తగిలిందనుకొన్నాడు వుంటే అతని కారణం లేచి స్టేషన్ గుమ్మంపై నిర్జీవంగా పడడంతోపాటు అడ్డొచ్చిన పోలీసు బలగాన్ని కూడా ఎదుర్కొని ఆ స్టేషన్ ని చిందరవందర చేసేవాడు వేసి వుండాలి పనులు చేసింది అపరంజి బొమ్మను చూసి ముగ్ధుడయ్యాడు తిరిగి కల్యాణి వస్తోంది ముఖాముఖీ వెళ్ళి గూడా లేనంత బిజీ అయిపోయాను చేరిన కడుపు నొప్పితో విలవిల లాడిపోతుంది అమలు జరపవచ్చునన్నది వుంది మాకు ఇది బయటపడదు\n"
     ]
    }
   ],
   "source": [
    "# Generate text\n",
    "input_text = \"ప్రేమ\"\n",
    "#input_text = \"మీరు ఇక్కడ ఉన్నందున ఇప్పుడు వదిలివేయవద్దు, తద్వారా ప్రపంచం మళ్లీ తనలాగే మారవచ్చు.\"\n",
    "generated_text2d = generate_sequence(input_text, model2, tokenizer, max_sequence_length, num_words=50, probabilistic=True)\n",
    "\n",
    "# Print the generated text\n",
    "print(generated_text2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "25c68860-836a-4a23-b1e4-c78a6fcd0318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If he had suffered a serious injury in a love affair, his reason would have been to get up and fall lifeless on the doorstep of the station, and to face the police force that intervened, and the person who ransacked the station should have done the work. He was impressed by the Aparanji doll. Does not come out\n"
     ]
    }
   ],
   "source": [
    "translated_text2d = translate_large_text(generated_text2d,\n",
    "                                        source_language=\"telugu\", target_language=\"english\")\n",
    "print(translated_text2d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4db47c9",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "This project successfully demonstrates the creation of a Telugu language chatbot using RNNs with LSTM units. By leveraging pre-trained word embeddings and training two different LSTM architectures, the model is capable of generating coherent Telugu text. The generated text highlights the model's ability to understand and predict language structures effectively, paving the way for advanced NLP applications in Telugu.\n",
    "\n",
    "However, it is important to note that the model's performance is constrained by the limited data and type of data it was trained on.. As a result, the range of emotions and vocabulary is also limited. Consequently, the model's predictions may sometimes loop within the restricted vocabulary provided. Expanding the dataset with more diverse and extensive text could further enhance the model's capabilities and robustness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1334dcd7",
   "metadata": {},
   "source": [
    "<div hidden>\n",
    "### Tokenizer Initialization\n",
    "    The Tokenizer() class is initialized to facilitate the tokenization process, a crucial step in natural language processing tasks like chatbot creation. It serves to convert textual data into numerical representations by assigning a unique integer index to each distinct word in the training dataset.\n",
    "\n",
    "### Sequence Conversion\n",
    "    After initializing the tokenizer, the fit_on_texts(training_data) method is employed to fit the tokenizer on the provided training data. This involves mapping words to their respective indices and building an internal vocabulary. Subsequently, the `texts_to_sequences(training_data)` function converts the original text data into sequences of tokens, replacing each word with its corresponding index.\n",
    "\n",
    "### Padding for Uniformity \n",
    "    To ensure uniformity in sequence lengths, the pad_sequences(sequences, maxlen=max_sequence_length) function is applied. This step involves padding or truncating the sequences to a specified maximum length (max_sequence_length). This is essential for inputting data into neural networks, as models typically expect fixed-size input sequences, and padding ensures consistent dimensions for efficient training and processing in the chatbot model.\n",
    "\n",
    "### Embedding\n",
    "    In the context of embedding, vocabulary size refers to the total number of unique words present in a given dataset or corpus. It is a crucial parameter when working with word embeddings, as it directly influences the size of the embedding matrix and the dimensionality of the word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab96b945-917c-4a6f-b5bf-07250a60c800",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
